# -*- coding: utf-8 -*-
"""Neural_network_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PkKxciuNz02ytoGnloDP9ZB0Ri85bMGq

In this project, we will be creating a GAN model that will produce realistic images, where our generator will create images while the discriminator will evaluate their authenticity

About our Dataset: we found our dataset on Kaggle, it is a 5000 high quality images' Real to Ghibli Dataset, the dataset contains two different folders, one with real images and one that contains artistic images in the style ghibli,

NB: the two folders of our dataset are independant and doesn't form a paired dataset so these are two distinct sets

We are willing to achieve too different ambitious objectives in this project :

**first objective** : we want to be able to create new ghibli images with our model from unseen images to the model

**second objective**: we would like to regenerate the original image by using the ghibli image

We chose the GAN because it is the most fit model for our project, and to achieve our objectives we will need to create what we call a **CycleGAN** which is basically two GANs composed from a generator 1 and discriminator 1 for the first task and generator 2 and discriminator 2 for the second task, we chose CycleGAN because this is unpaired data so we are kind of obligated to do that :)
"""

#!pip install tensorflow keras
#!pip install tensorflow-addons

# Standard libraries
import os
import zipfile

# Image and visualization
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# TensorFlow and Keras
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import LayerNormalization
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input

zip_file_path = '/content/dataset.zip'
extract_dir = '/dataset'
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# paths to the extracted directories
real_image_dir = '/dataset/dataset/trainA'
ghibli_image_dir = '/dataset/dataset/trainB_ghibli'
#files in each directory
real_files = os.listdir(real_image_dir)
ghibli_files = os.listdir(ghibli_image_dir)

# basic info (number of images, size, mode) for the images
def describe_images(image_dir, indices):
    files = os.listdir(image_dir)
    image_count = len(files)  #number of images
    image_details = []
    for index in indices:
        if index < image_count:
            image_path = os.path.join(image_dir, files[index])
            with Image.open(image_path) as img:
                width, height = img.size
                mode = img.mode  # RGB or grayscale
                image_details.append((index, width, height, mode))
        else:
            image_details.append((index, None, None, "Index out of range"))

    return image_count, image_details

indices_to_check = [0, 599, 1869]  # Image 1, 600, 1870
# Describing the real images directory
real_image_count, real_image_details = describe_images(real_image_dir, indices_to_check)
# Describing the Ghibli images directory
ghibli_image_count, ghibli_image_details = describe_images(ghibli_image_dir, indices_to_check)

print(f"Real Images: {real_image_count} images")
for detail in real_image_details:
    print(f"Image {detail[0] + 1}: Size: {detail[1]}x{detail[2]}, Mode: {detail[3]}")
print("\n")
print(f"Ghibli Images: {ghibli_image_count} images")
for detail in ghibli_image_details:
    print(f"Image {detail[0] + 1}: Size: {detail[1]}x{detail[2]}, Mode: {detail[3]}")

"""The number of the images in each of the two folders of our dataset is the same, both are composed of 2500 images in total, which is great because we will have a balanced dataset, allowing the CycleGAN to learn from an equal number of images from both domains during training.

we displayed randomly indexs of images for each of the two folders to see how the data is composed and how the images are shaped, we can see that the widths and the heights are quite different from an image to another, we can find similarities but mostly we can't work with such different shapes of our images (we will standarize the shape).

As you can see we also displayed the mode of our images, so the images are RGB meaning they have three channels(are colored basically).
"""

# the first 3 images from both directories
real_images = [mpimg.imread(os.path.join(real_image_dir, real_files[i])) for i in range(3)]
ghibli_images = [mpimg.imread(os.path.join(ghibli_image_dir, ghibli_files[i])) for i in range(3)]
fig, axes = plt.subplots(2, 3, figsize=(12, 6))
axes = axes.flatten()
print("here are some samples of our dataset: ")
for i in range(3):
    axes[i].imshow(real_images[i])
    axes[i].axis('off')
    axes[i].set_title(f'Real Image {i+1}')
for i in range(3):
    axes[i + 3].imshow(ghibli_images[i])
    axes[i + 3].axis('off')
    axes[i + 3].set_title(f'Ghibli Image {i+1}')
plt.tight_layout()
plt.show()

"""as we can see The real images' dataset shows real world images, and the ghibli one shows artistic Ghibli images.

**Preprocessing data**

In this part, we will prepare our dataset to be used in our models, we need to resize the images, normalize them and we will also convert them into tensors, we need them to be in tensor format so we can feed them to our neural network,we chose to use the **TensorFlow version of the GAN from Lab 3** because its structure aligned better with how we wanted to build and organize our GAN. Both PyTorch and TensorFlow were available, but TensorFlow allowed us to keep the code clean and focused on what we wante to demonstrate.

**Recap of the jobs of our preprocessing Pipeline:** will load, resize, normalize, convert to tensor format, batch the data (for training), and augmentate the data(increasing the diversity of the training set for better generalisation)
"""

def preprocessing_pipeline(real_dir, ghibli_dir, img_size=(256, 256), batch_size=16, augment=True):
    # Step 1: Loading image function
    def load_image_paths(directory):
        return [os.path.join(directory, f) for f in os.listdir(directory)]

    real_image_paths = load_image_paths(real_dir)
    ghibli_image_paths = load_image_paths(ghibli_dir)

    # Step 2: image preprocessing function
    def preprocess_image(image_path):
        img = tf.io.read_file(image_path)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, img_size)
        img = (img / 127.5) - 1  # we normalize to [-1, 1]
        return img
    # Step 3: Data augmentation function we can set it to false if xe don't want it,
    # we will be trying both in our code to see what improvement it can have on our generalization
    def preprocess_and_augment(image_path):
        img = preprocess_image(image_path)
        if augment:
            img = tf.image.random_flip_left_right(img)
        return img

    # Step 4: creates shuffled, batched, and preprocessed TensorFlow dataset objects.
    # ready to be fed to the GAN for training.
    real_ds = tf.data.Dataset.from_tensor_slices(real_image_paths)\
        .map(preprocess_and_augment, num_parallel_calls=tf.data.AUTOTUNE)\
        .shuffle(1000)\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    ghibli_ds = tf.data.Dataset.from_tensor_slices(ghibli_image_paths)\
        .map(preprocess_and_augment, num_parallel_calls=tf.data.AUTOTUNE)\
        .shuffle(1000)\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    # Step 5: since we are creating a CycleGAN, the model will need a batch of each of the two datasets at every training step so we zip them together
    return tf.data.Dataset.zip((real_ds, ghibli_ds))

# using the pipeline on our dataset
gan_dataset = preprocessing_pipeline(real_image_dir, ghibli_image_dir)

"""this next step is purely for verifying, inspecting, and documenting the outputs, we want to make sure that our pipeline is working as expected and we also want to compare the differences between the images after the transformation, for that we display 2 batchs of our dataset, we unormalize the second batch just to be sure that the pixels were really well transformed, see the interpration after the output :)"""

# we take one batch from the GAN dataset
for real_batch, ghibli_batch in gan_dataset.take(1):
    # -------------------NORMALIZED STATS ------------------- #
    print(" REAL IMAGES (NORMALIZED):")
    print("- Shape:", real_batch.shape)
    print("- Pixel range: min =", tf.reduce_min(real_batch).numpy(), ", max =", tf.reduce_max(real_batch).numpy())
    print("- Sample pixel (real[0][0][0]):", real_batch[0][0][0].numpy())

    print("\n GHIBLI IMAGES (NORMALIZED):")
    print("- Shape:", ghibli_batch.shape)
    print("- Pixel range: min =", tf.reduce_min(ghibli_batch).numpy(), ", max =", tf.reduce_max(ghibli_batch).numpy())
    print("- Sample pixel (ghibli[0][0][0]):", ghibli_batch[0][0][0].numpy())

    # -------------------NORMALIZED ([-1, 1]) ------------------- #
    fig, axes = plt.subplots(2, 3, figsize=(12, 6))
    for i in range(3):
        axes[0, i].imshow(real_batch[i])  # directly using normalized tensors
        axes[0, i].axis('off')
        axes[0, i].set_title(f'Normalized Real {i+1}')

        axes[1, i].imshow(ghibli_batch[i])
        axes[1, i].axis('off')
        axes[1, i].set_title(f'Normalized Ghibli {i+1}')
    plt.suptitle(" Normalized Images (-1 to 1)")
    plt.tight_layout()
    plt.show()

    # -------------------UNNORMALIZED ([0, 1]) ------------------- #
    real_batch_disp = (real_batch + 1) / 2.0
    ghibli_batch_disp = (ghibli_batch + 1) / 2.0

    fig, axes = plt.subplots(2, 3, figsize=(12, 6))
    for i in range(3):
        axes[0, i].imshow(real_batch_disp[i])
        axes[0, i].axis('off')
        axes[0, i].set_title(f'Real {i+1}')

        axes[1, i].imshow(ghibli_batch_disp[i])
        axes[1, i].axis('off')
        axes[1, i].set_title(f'Ghibli {i+1}')
    plt.suptitle(" Unnormalized Images (0 to 1)")
    plt.tight_layout()
    plt.show()
        # ------------------- UNNORMALIZED STATS ([0, 1]) ------------------- #
    print("\nREAL IMAGES (UNNORMALIZED):")
    print("- Shape:", real_batch_disp.shape)
    print("- Pixel range: min =", tf.reduce_min(real_batch_disp).numpy(), ", max =", tf.reduce_max(real_batch_disp).numpy())
    print("- Sample pixel (real_disp[0][0][0]):", real_batch_disp[0][0][0].numpy())

    print("\nGHIBLI IMAGES (UNNORMALIZED):")
    print("- Shape:", ghibli_batch_disp.shape)
    print("- Pixel range: min =", tf.reduce_min(ghibli_batch_disp).numpy(), ", max =", tf.reduce_max(ghibli_batch_disp).numpy())
    print("- Sample pixel (ghibli_disp[0][0][0]):", ghibli_batch_disp[0][0][0].numpy())
    break

"""regarding the shapes we can see clearly that now we have a uniformed size for all the images, as for the pixel range, it was well normalized for the images we are going to use in the training, the sample shows either -1 or 1 for each pixel, and we can clearly see on the images that they are more dark,and unclear for us humans (easier for the model though) and the second display which unnormalized meaning pixels are in [0,1] shows colors that are more visible and natural if we can say that so it is easier to understand visually for a human

Splitting the data
"""

# Count total batches
dataset_size = sum(1 for _ in gan_dataset)

# Define split ratios
train_ratio = 0.8
val_ratio = 0.1

# Calculate sizes
train_size = int(dataset_size * train_ratio)
val_size = int(dataset_size * val_ratio)

# Split the dataset
train_dataset = gan_dataset.take(train_size)
val_dataset = gan_dataset.skip(train_size).take(val_size)
test_dataset = gan_dataset.skip(train_size + val_size)

# Confirm
print(f"Train: {train_size} batches, Val: {val_size} batches, Test: {dataset_size - train_size - val_size} batches")

"""# **GAN (real → ghibli)**

we define these two functions first because we'll reuse them to build both our encoder and decoder blocks

in downsample(), we use Conv2D to reduce spatial size, optional LayerNorm to stabilize training, and LeakyReLU to keep gradients flowing(avoid vanishing gradient)

in upsample(), we use Conv2DTranspose to increase size, LayerNorm again, optional Dropout for regularization, and ReLU for activation
"""

"""
We chose LayerNormalization instead of BatchNormalization in our generator blocks because LayerNorm is independent of the batch size and normalizes across the features of each sample.
It can lead to more stable training, especially when using small batches or when the network needs to preserve instance level details,
which is often the case in image-to-image translation tasks like CycleGAN.
"""
def downsample(filters, size, apply_norm=True):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(layers.Conv2D(filters, size, strides=2, padding='same',
                              kernel_initializer=initializer, use_bias=not apply_norm))
    if apply_norm:
        result.add(LayerNormalization())
    result.add(layers.LeakyReLU())
    return result

def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',
                                      kernel_initializer=initializer, use_bias=False))
    result.add(LayerNormalization())
    if apply_dropout:
        result.add(layers.Dropout(0.5))
    result.add(layers.ReLU())
    return result

"""**Generator 1 : Real to Ghibli**"""

"""
We used `tanh` as the activation function in the final layer of our generators because our image pixel values were normalized to the range [-1, 1].
The `tanh` function outputs values in exactly that range, making it a natural choice for this setup and improving convergence during training.
"""
def build_generator_rg():
    inputs = layers.Input(shape=[256, 256, 3])

    # Encoder
    down_stack = [
        downsample(64, 4, apply_norm=False),
        downsample(128, 4),
        downsample(256, 4),
        downsample(512, 4),
        downsample(512, 4),
        downsample(512, 4),
        downsample(512, 4),
        downsample(512, 4),
    ]

    # Decoder
    up_stack = [
        upsample(512, 4, apply_dropout=True),
        upsample(512, 4, apply_dropout=True),
        upsample(512, 4, apply_dropout=True),
        upsample(512, 4),
        upsample(256, 4),
        upsample(128, 4),
        upsample(64, 4),
    ]

    initializer = tf.random_normal_initializer(0., 0.02)
    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same',
                                  kernel_initializer=initializer, activation='tanh')

    x = inputs
    skips = []
    for down in down_stack:
        x = down(x)
        skips.append(x)

    skips = reversed(skips[:-1])
# We use a uet architecture with concatenation skip connections.
# This allows the decoder to access high-resolution features from the encoder,
# preserving spatial details and improving image reconstruction quality.

    for up, skip in zip(up_stack, skips):
        x = up(x)
        x = layers.Concatenate()([x, skip])

    x = last(x)

    return tf.keras.Model(inputs=inputs, outputs=x, name="G_RG")

G_RG = build_generator_rg()
G_RG.summary()

# Use a batch from the training split instead of full dataset
for real_batch, _ in train_dataset.take(1):
    generated_ghibli = G_RG(real_batch, training=False)
    break

# Function to display results
def display_images(real, generated, num=3):
    real = (real + 1) / 2.0  # denormalize
    generated = (generated + 1) / 2.0
    plt.figure(figsize=(12, 4))
    for i in range(num):
        plt.subplot(2, num, i+1)
        plt.imshow(real[i])
        plt.title("Image Réelle")
        plt.axis("off")

        plt.subplot(2, num, i+1+num)
        plt.imshow(generated[i])
        plt.title("Image Ghibli générée")
        plt.axis("off")
    plt.tight_layout()
    plt.show()

# Showing results
display_images(real_batch, generated_ghibli)

print(tf.reduce_min(generated_ghibli).numpy(), tf.reduce_max(generated_ghibli).numpy())

"""**Discriminator 1 : for Ghibli images**"""

"""
Our discriminators follow the PatchGAN architecture, which evaluates the realism of local image patches (for example 70x70 patches) rather than the entire image at once.
This has been shown to encourage sharper and more detailed results, as it forces the generator to focus on producing locally consistent textures and structures.
"""
def build_discriminator_g():
    initializer = tf.random_normal_initializer(0., 0.02)

    inp = layers.Input(shape=[256, 256, 3], name='input_image')

    x = layers.Conv2D(64, 4, strides=2, padding='same',
                      kernel_initializer=initializer)(inp)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2D(128, 4, strides=2, padding='same',
                      kernel_initializer=initializer, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2D(256, 4, strides=2, padding='same',
                      kernel_initializer=initializer, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2D(512, 4, strides=1, padding='same',
                      kernel_initializer=initializer, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    # last layer one output (patch probability )
    x = layers.Conv2D(1, 4, strides=1, padding='same',
                      kernel_initializer=initializer)(x)

    return tf.keras.Model(inputs=inp, outputs=x, name="D_G")

D_G = build_discriminator_g()
D_G.summary()

for real_batch, ghibli_batch in train_dataset.take(1):
    real_sample = real_batch[0:1]    # Shape: (1, 256, 256, 3)
    ghibli_sample = ghibli_batch[0:1]
    break

fake_ghibli = G_RG(real_sample, training=False)

real_output = D_G(ghibli_sample, training=False)
fake_output = D_G(fake_ghibli, training=False)

print("Sortie sur vraie image Ghibli :", real_output.shape)
print("Extrait valeurs réelles :", tf.reduce_mean(real_output).numpy())

print("Sortie sur image Ghibli générée :", fake_output.shape)
print("Extrait valeurs générées :", tf.reduce_mean(fake_output).numpy())

"""**loss for GAN 1**"""

loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Adversarial loss (to fool the discriminator)
def generator_loss_gan(fake_output):
    return loss_obj(tf.ones_like(fake_output), fake_output)

# Discriminator loss (so we recognize real from ghibli)
def discriminator_loss(real_output, fake_output):
    real_loss = loss_obj(tf.ones_like(real_output), real_output)
    fake_loss = loss_obj(tf.zeros_like(fake_output), fake_output)
    return (real_loss + fake_loss) * 0.5

# Cycle-consistency loss
def cycle_consistency_loss(real_image, cycled_image, LAMBDA=10):
    return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))

# Identity loss (to stabilize)
def identity_loss(real_image, same_image, LAMBDA=5):
    return LAMBDA * tf.reduce_mean(tf.abs(real_image - same_image))

"""**Optimizers**"""

generator_rg_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

"""**Partial training for the first couple G_RG/D_G**

****

**AND loss for GAN 1**
"""

@tf.function
def train_step_rg(real_images, ghibli_images):
    with tf.GradientTape(persistent=True) as tape:
        # ghibli generation
        fake_ghibli = G_RG(real_images, training=True)

        # Discriminations
        disc_real = D_G(ghibli_images, training=True)
        disc_fake = D_G(fake_ghibli, training=True)

        # loss computing
        gen_loss = generator_loss_gan(disc_fake)
        disc_loss = discriminator_loss(disc_real, disc_fake)

    # gradients application
    generator_gradients = tape.gradient(gen_loss, G_RG.trainable_variables)
    discriminator_gradients = tape.gradient(disc_loss, D_G.trainable_variables)

    generator_rg_optimizer.apply_gradients(zip(generator_gradients, G_RG.trainable_variables))
    discriminator_g_optimizer.apply_gradients(zip(discriminator_gradients, D_G.trainable_variables))

    return gen_loss, disc_loss

"""**Visualizing results through the pretaining process**

Before training the full CycleGAN model, we pretrain both G_RG (real to ghibli) and G_GR (ghibli to real) with their respective discriminators. This "warm-up" helps both generators start from a reasonable baseline, making the joint training more stable. Without this, the discriminators might overpower the generators early on, leading to poor convergence or mode collapse.
"""

# shows real image and its stylized version (from G_RG)
def generate_and_show_images(generator, test_input, epoch):
    prediction = generator(test_input, training=False)

    # denormalization (from [-1, 1] à [0, 1])
    prediction = (prediction + 1) / 2.0
    test_input_visu = (test_input + 1) / 2.0

    plt.figure(figsize=(6, 3))

    plt.subplot(1, 2, 1)
    plt.imshow(test_input_visu[0])
    plt.title('real image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(prediction[0])
    plt.title(f'ghibli generated (epoch {epoch})')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

"""
As you will see through the entire training processes we implemented, each time we save the model weights to ensure we don't lose progress if training is interrupted.
It also allows us to resume training later or evaluate intermediate models.
"""
EPOCHS = 300

# to have a trace of the losses
gen_losses = []
disc_losses = []

for epoch in range(EPOCHS):
    print(f"\n Epoch {epoch+1}/{EPOCHS}")

    total_gen_loss = 0
    total_disc_loss = 0
    batch_count = 0

    for step, (real_batch, ghibli_batch) in enumerate(train_dataset):
        gen_loss, disc_loss = train_step_rg(real_batch, ghibli_batch)
        total_gen_loss += gen_loss
        total_disc_loss += disc_loss
        batch_count += 1

    # Mean losses
    avg_gen_loss = total_gen_loss / batch_count
    avg_disc_loss = total_disc_loss / batch_count
    gen_losses.append(avg_gen_loss)
    disc_losses.append(avg_disc_loss)

    print(f"gen_loss: {avg_gen_loss:.4f} | disc_loss: {avg_disc_loss:.4f}")

    # Visualization every 5 epochs using a fixed real batch from training set
    if (epoch + 1) % 5 == 0:
        sample_real = next(iter(val_dataset))[0]
        generated_ghibli = G_RG(sample_real, training=False)

        real_disp = (sample_real[0] + 1) / 2.0
        ghibli_disp = (generated_ghibli[0] + 1) / 2.0

        import matplotlib.pyplot as plt
        plt.figure(figsize=(8, 4))
        plt.subplot(1, 2, 1)
        plt.imshow(real_disp)
        plt.axis('off')
        plt.title("real image")

        plt.subplot(1, 2, 2)
        plt.imshow(ghibli_disp)
        plt.axis('off')
        plt.title(f"Ghibli generated (epoch {epoch+1})")

        plt.suptitle(f"result of {epoch+1}")
        plt.tight_layout()
        plt.show()

    # Save models every 10 epochs
    if (epoch + 1) % 10 == 0:
        G_RG.save(f"G_RG_epoch_{epoch+1}.h5")
        D_G.save(f"D_G_epoch_{epoch+1}.h5")
        print(f" models G_RG et D_G saved at epoch {epoch+1}")

"""# **GAN (ghibli → real)**

**Generator 2 : Ghibli to Real**
"""

def build_generator_gr():
    inputs = layers.Input(shape=[256, 256, 3])
    down_stack = [
        downsample(64, 4, apply_norm=False),
        downsample(128, 4),
        downsample(256, 4),
        downsample(512, 4),
        downsample(512, 4),
        downsample(512, 4),
        downsample(512, 4),
        downsample(512, 4),
    ]
    up_stack = [
        upsample(512, 4, apply_dropout=True),
        upsample(512, 4, apply_dropout=True),
        upsample(512, 4, apply_dropout=True),
        upsample(512, 4),
        upsample(256, 4),
        upsample(128, 4),
        upsample(64, 4),
    ]
    initializer = tf.random_normal_initializer(0., 0.02)
    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same',
                                  kernel_initializer=initializer, activation='tanh')
    x = inputs
    skips = []
    for down in down_stack:
        x = down(x)
        skips.append(x)
    skips = reversed(skips[:-1])
    for up, skip in zip(up_stack, skips):
        x = up(x)
        x = layers.Concatenate()([x, skip])
    x = last(x)
    return tf.keras.Model(inputs=inputs, outputs=x, name="G_GR")

G_GR = build_generator_gr()
G_GR.summary()

"""**Discriminator 2 : for Real images**"""

def build_discriminator_r():
    initializer = tf.random_normal_initializer(0., 0.02)
    inp = layers.Input(shape=[256, 256, 3], name='input_image')
    x = layers.Conv2D(64, 4, strides=2, padding='same', kernel_initializer=initializer)(inp)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(128, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(256, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(512, 4, strides=1, padding='same', kernel_initializer=initializer, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(1, 4, strides=1, padding='same', kernel_initializer=initializer)(x)
    return tf.keras.Model(inputs=inp, outputs=x, name="D_R")

D_R = build_discriminator_r()
D_R.summary()

"""**Optimizers**"""

generator_gr_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_r_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

"""**Partial training for the second couple  G_GR and D_R**

****

**AND loss for GAN 2**
"""

@tf.function
def train_step_gr(ghibli_images, real_images):
    with tf.GradientTape(persistent=True) as tape:
        fake_real = G_GR(ghibli_images, training=True)

        disc_real = D_R(real_images, training=True)
        disc_fake = D_R(fake_real, training=True)

        gen_loss = generator_loss_gan(disc_fake)
        disc_loss = discriminator_loss(disc_real, disc_fake)

    generator_gradients = tape.gradient(gen_loss, G_GR.trainable_variables)
    discriminator_gradients = tape.gradient(disc_loss, D_R.trainable_variables)

    generator_gr_optimizer.apply_gradients(zip(generator_gradients, G_GR.trainable_variables))
    discriminator_r_optimizer.apply_gradients(zip(discriminator_gradients, D_R.trainable_variables))

    return gen_loss, disc_loss

"""**Visualization**"""

def display_ghibli_to_real(ghibli, generated, num=3):
    ghibli = (ghibli + 1) / 2.0
    generated = (generated + 1) / 2.0
    plt.figure(figsize=(12, 4))
    for i in range(num):
        plt.subplot(2, num, i+1)
        plt.imshow(ghibli[i])
        plt.title("Ghibli image")
        plt.axis("off")
        plt.subplot(2, num, i+1+num)
        plt.imshow(generated[i])
        plt.title("real image generated")
        plt.axis("off")
    plt.tight_layout()
    plt.show()

EPOCHS = 300

for epoch in range(EPOCHS):
    print(f"\n [GR Epoch {epoch+1}/{EPOCHS}]")
    total_gen_loss = 0
    total_disc_loss = 0
    steps = 0

    for real_batch, ghibli_batch in train_dataset:
        gen_loss, disc_loss = train_step_gr(ghibli_batch, real_batch)
        total_gen_loss += gen_loss
        total_disc_loss += disc_loss
        steps += 1

    avg_gen = total_gen_loss / steps
    avg_disc = total_disc_loss / steps
    print(f" gen_loss: {avg_gen:.4f} | disc_loss: {avg_disc:.4f}")

    # we visualize every 5 epochs to see the progress
    if (epoch + 1) % 5 == 0:
        for _, ghibli_sample in val_dataset.take(1):
            fake_real = G_GR(ghibli_sample, training=False)
            display_ghibli_to_real(ghibli_sample, fake_real)

    # we save every 50 epochs foir security
    if (epoch + 1) % 50 == 0:
        G_GR.save(f"G_GR_epoch_{epoch+1}.h5")
        D_R.save(f"D_R_epoch_{epoch+1}.h5")
        print(f"models G_GR et D_R saved at epoch {epoch+1}")

"""**VGG19 perceptual loss**"""

vgg = VGG19(include_top=False, weights='imagenet')
vgg.trainable = False
feature_extractor = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)

# perceptual loss
def perceptual_loss(real, generated):
    real = (real + 1) * 127.5
    generated = (generated + 1) * 127.5
    real_preprocessed = preprocess_input(real)
    generated_preprocessed = preprocess_input(generated)
    real_features = feature_extractor(real_preprocessed)
    generated_features = feature_extractor(generated_preprocessed)
    return tf.reduce_mean(tf.abs(real_features - generated_features))

"""
After looking up some improvements to implement to our model in order to have a better output,
we found a feature loss model that has been pretrained on ImageNet, it is called VGG19 network,
it's used for classification normally but we will use it so it can know if the images generated looks like real world images or no to it,
it is a kind of transfer learning, we think more of knowledge transfer because we're not really training the VGG19 but using it to help our GAN learn better,
we made this decision after realizing it is hard to reconstruct lost information.
We included perceptual loss to encourage the generators to produce images that are not only pixel-wise accurate but also perceptually realistic.
VGG19 extracts high level features from images, so comparing these features helps guide the model to generate outputs that align with human visual perception.
"""

"""#**GYCLEGAN FULL TRAINING**"""

# LAMBDA_CYCLE controls how strongly the model is penalized if it fails to reconstruct the original image after a cycle (for examaple Real to Ghibli to Real).
# LAMBDA_ID controls how much the model is penalized for altering an image that is already in the target domain to stabilize training.
#we started with 10 and 5 respectively but we didn't have the best outputs, so we decided to increase both
LAMBDA_CYCLE = 20
LAMBDA_ID = 10
LAMBDA_PERCEP = 1.0
# training function with both RG and GR with cycle-consistency
@tf.function
def train_step_cyclegan(real_images, ghibli_images):
    with tf.GradientTape(persistent=True) as tape:
        # Generations
        fake_ghibli = G_RG(real_images, training=True)
        fake_real = G_GR(ghibli_images, training=True)

        # Cycles
        cycled_real = G_GR(fake_ghibli, training=True)
        cycled_ghibli = G_RG(fake_real, training=True)

        # Identities
        same_real = G_GR(real_images, training=True)
        same_ghibli = G_RG(ghibli_images, training=True)

        # Discriminations
        disc_real_g = D_G(ghibli_images, training=True)
        disc_fake_g = D_G(fake_ghibli, training=True)
        disc_real_r = D_R(real_images, training=True)
        disc_fake_r = D_R(fake_real, training=True)

        # adversial losses
        gen_rg_loss = generator_loss_gan(disc_fake_g)
        gen_gr_loss = generator_loss_gan(disc_fake_r)

        # cycle-consistency losses
        cycle_loss = cycle_consistency_loss(real_images, cycled_real, LAMBDA_CYCLE) + \
                     cycle_consistency_loss(ghibli_images, cycled_ghibli, LAMBDA_CYCLE)

        # idendity losses
        id_loss_rg = identity_loss(ghibli_images, same_ghibli, LAMBDA_ID)
        id_loss_gr = identity_loss(real_images, same_real, LAMBDA_ID)

        # perceptual losses
        percep_loss_rg = perceptual_loss(ghibli_images, fake_ghibli)
        percep_loss_gr = perceptual_loss(real_images, fake_real)

        # losses
        total_gen_rg_loss = gen_rg_loss + cycle_loss + id_loss_rg + LAMBDA_PERCEP * percep_loss_rg
        total_gen_gr_loss = gen_gr_loss + cycle_loss + id_loss_gr + LAMBDA_PERCEP * percep_loss_gr

        disc_g_loss = discriminator_loss(disc_real_g, disc_fake_g)
        disc_r_loss = discriminator_loss(disc_real_r, disc_fake_r)

    # optimization with gradients
    G_RG_grad = tape.gradient(total_gen_rg_loss, G_RG.trainable_variables)
    G_GR_grad = tape.gradient(total_gen_gr_loss, G_GR.trainable_variables)
    D_G_grad = tape.gradient(disc_g_loss, D_G.trainable_variables)
    D_R_grad = tape.gradient(disc_r_loss, D_R.trainable_variables)

    generator_rg_optimizer.apply_gradients(zip(G_RG_grad, G_RG.trainable_variables))
    generator_gr_optimizer.apply_gradients(zip(G_GR_grad, G_GR.trainable_variables))
    discriminator_g_optimizer.apply_gradients(zip(D_G_grad, D_G.trainable_variables))
    discriminator_r_optimizer.apply_gradients(zip(D_R_grad, D_R.trainable_variables))

    return total_gen_rg_loss, total_gen_gr_loss, disc_g_loss, disc_r_loss

def display_cycle_images(real, fake_ghibli, cycled_real, ghibli, fake_real, cycled_ghibli, num=2):
    real = (real + 1) / 2.0
    fake_ghibli = (fake_ghibli + 1) / 2.0
    cycled_real = (cycled_real + 1) / 2.0
    ghibli = (ghibli + 1) / 2.0
    fake_real = (fake_real + 1) / 2.0
    cycled_ghibli = (cycled_ghibli + 1) / 2.0

    plt.figure(figsize=(12, 6))
    for i in range(num):
        plt.subplot(3, num, i+1)
        plt.imshow(real[i])
        plt.axis("off")
        plt.title("Réel")

        plt.subplot(3, num, i+1+num)
        plt.imshow(fake_ghibli[i])
        plt.axis("off")
        plt.title("→ Ghibli")

        plt.subplot(3, num, i+1+2*num)
        plt.imshow(cycled_real[i])
        plt.axis("off")
        plt.title("↺ Réel")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(12, 6))
    for i in range(num):
        plt.subplot(3, num, i+1)
        plt.imshow(ghibli[i])
        plt.axis("off")
        plt.title("Ghibli")

        plt.subplot(3, num, i+1+num)
        plt.imshow(fake_real[i])
        plt.axis("off")
        plt.title("→ Réel")

        plt.subplot(3, num, i+1+2*num)
        plt.imshow(cycled_ghibli[i])
        plt.axis("off")
        plt.title("↺ Ghibli")
    plt.tight_layout()
    plt.show()

# we will store CycleGAN losses per epoch
cycle_g_rg_losses = []
cycle_g_gr_losses = []
cycle_d_g_losses = []
cycle_d_r_losses = []


EPOCHS = 300
G_RG.load_weights("G_RG_epoch_300.h5")
D_G.load_weights("D_G_epoch_300.h5")
G_GR.load_weights("G_GR_epoch_300.h5")
D_R.load_weights("D_R_epoch_300.h5")
print(" preentrained models charger before the training of CycleGAN.")

# loop for training the CycleGAN using the preentrained
for epoch in range(EPOCHS):
    print(f"\n [CycleGAN Epoch {epoch+1}/{EPOCHS}]")

    total_g_rg_loss = 0
    total_g_gr_loss = 0
    total_d_g_loss = 0
    total_d_r_loss = 0

    for real_batch, ghibli_batch in train_dataset:
        g_rg_loss, g_gr_loss, d_g_loss, d_r_loss = train_step_cyclegan(real_batch, ghibli_batch)
        total_g_rg_loss += g_rg_loss
        total_g_gr_loss += g_gr_loss
        total_d_g_loss += d_g_loss
        total_d_r_loss += d_r_loss

    n_batches = tf.data.experimental.cardinality(train_dataset).numpy()
    print(f" G_RG Loss: {total_g_rg_loss/n_batches:.4f} | G_GR Loss: {total_g_gr_loss/n_batches:.4f}")
    print(f" D_G  Loss: {total_d_g_loss/n_batches:.4f} | D_R  Loss: {total_d_r_loss/n_batches:.4f}")

    cycle_g_rg_losses.append(total_g_rg_loss / n_batches)
    cycle_g_gr_losses.append(total_g_gr_loss / n_batches)
    cycle_d_g_losses.append(total_d_g_loss / n_batches)
    cycle_d_r_losses.append(total_d_r_loss / n_batches)

    # every 10 epochs we want the output to be able to visualize the progress
    if (epoch + 1) % 10 == 0:
        real_batch, ghibli_batch = next(iter(val_dataset))
        fake_ghibli = G_RG(real_batch, training=False)
        cycled_real = G_GR(fake_ghibli, training=False)
        fake_real = G_GR(ghibli_batch, training=False)
        cycled_ghibli = G_RG(fake_real, training=False)

        display_cycle_images(real_batch, fake_ghibli, cycled_real, ghibli_batch, fake_real, cycled_ghibli)

    # we save every 50 epochs in case the code crashs or if we need to stop the running code so we can have the last output saved
    if (epoch + 1) % 50 == 0:
        G_RG.save(f"G_RG_cycle_epoch_{epoch+1}.h5")
        G_GR.save(f"G_GR_cycle_epoch_{epoch+1}.h5")
        D_G.save(f"D_G_cycle_epoch_{epoch+1}.h5")
        D_R.save(f"D_R_cycle_epoch_{epoch+1}.h5")
        print(f" models saved at epoch {epoch+1}")

        # # Plotting and saving the loss evolution curves for the 4 CycleGAN components
        plt.figure(figsize=(10, 6))
        plt.plot(cycle_g_rg_losses, label="G_RG Loss")
        plt.plot(cycle_g_gr_losses, label="G_GR Loss")
        plt.plot(cycle_d_g_losses, label="D_G Loss")
        plt.plot(cycle_d_r_losses, label="D_R Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title(f"CycleGAN Losses up to Epoch {epoch+1}")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()

        # Saving the plot to disk for later visualization of training progress
        plt.savefig(f"cycle_losses_epoch_{epoch+1}.png")

demo_dir = "demo_outputs"
os.makedirs(demo_dir, exist_ok=True)

# Taking a batch of test_dataset
for test_real_batch, test_ghibli_batch in test_dataset.take(1):
    # Generation
    fake_ghibli = G_RG(test_real_batch, training=False)
    cycled_real = G_GR(fake_ghibli, training=False)

    fake_real = G_GR(test_ghibli_batch, training=False)
    cycled_ghibli = G_RG(fake_real, training=False)

    # Denormalization
    test_real_disp = (test_real_batch + 1) / 2.0
    fake_ghibli_disp = (fake_ghibli + 1) / 2.0
    cycled_real_disp = (cycled_real + 1) / 2.0

    test_ghibli_disp = (test_ghibli_batch + 1) / 2.0
    fake_real_disp = (fake_real + 1) / 2.0
    cycled_ghibli_disp = (cycled_ghibli + 1) / 2.0

    # Showing 3 examples and saving them
    for i in range(3):
        fig, axes = plt.subplots(2, 3, figsize=(12, 6))

        # Line 1 : Real → Ghibli → Real
        axes[0, 0].imshow(test_real_disp[i])
        axes[0, 0].set_title("Image Réelle")
        axes[0, 1].imshow(fake_ghibli_disp[i])
        axes[0, 1].set_title("Fake Ghibli")
        axes[0, 2].imshow(cycled_real_disp[i])
        axes[0, 2].set_title("Cycle Real")

        # Line 2 : Ghibli → Real → Ghibli
        axes[1, 0].imshow(test_ghibli_disp[i])
        axes[1, 0].set_title("Image Ghibli")
        axes[1, 1].imshow(fake_real_disp[i])
        axes[1, 1].set_title("Fake Réel")
        axes[1, 2].imshow(cycled_ghibli_disp[i])
        axes[1, 2].set_title("Cycle Ghibli")

        for ax in axes.flatten():
            ax.axis("off")

        plt.tight_layout()
        plt.savefig(os.path.join(demo_dir, f"demo_sample_{i+1}.png"))
        plt.show()

"""After starting to train our CycleGAN, we can observe that the transformation from real world images to the Ghibli style produced visually coherent and convincing results. The generated images successfully captured the artistic essence of the Ghibli domain.

 However, the reverse transformation from Ghibli to real proved to be much more challenging. We noticed that the outputs were imprecise and lacked realistic detail. we can only guess that this is due to the asymmetry of the task because going from real to Ghibli involves stylizing and simplifying details while the reverse requires reconstructing complex, high resolution information from stylized inputs. Since Ghibli images are artistic abstractions of the real world, trying to go from a simplified image to a complex real world scene requires generating realistic details that were never present in the input. This makes the task significantly more challenging and less reliable.
"""

#@title Convert ipynb to HTML in Colab
# Upload ipynb
from google.colab import files
f = files.upload()

# Convert ipynb to html
import subprocess
file0 = list(f.keys())[0]
_ = subprocess.run(["pip", "install", "nbconvert"])
_ = subprocess.run(["jupyter", "nbconvert", file0, "--to", "html"])

# download the html
files.download(file0[:-5]+"html")